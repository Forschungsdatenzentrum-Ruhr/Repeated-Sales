non_list_duration < 0, "Miss",
non_list_duration >= 0 & non_list_duration <= 6, "Update",
non_list_duration > 6, "Sold"
))
]
# set index to allow for binary operations here and later
setindex(parent_grouped_data_non_list, non_list_reason)
parent_grouped_data_connected <- parent_grouped_data_non_list[
# non_list_reason ==
!"Miss",
c("start_position", "end_position") := which_range(non_list_reason),
on = "non_list_reason"
][
,
# can calculate something like price difference from initial offering to actual sale price
amonths := fifelse(
!start_position == end_position & !is.na(start_position),
amonths[start_position],
amonths
)
]
return(parent_grouped_data_connected)
# parent_grouped_data
}
# .SDcols = c("amonths","emonths")
geo_grouped_data_connected <- geo_grouped_data_similarity[, non_list_classification(.SD, data_end_date), by = parent][!"Update", on = "non_list_reason"]
View(geo_grouped_data_connected)
View(geo_grouped_data_connected)
tar_make()
tar_make()
# General TODO: -----------------------------------------------------------
# use formatting of pop-forecast
# swap away from %>% ?
# add packagename before functions
# use data.table
# cluster around each baseline?
# Packages-Setup: ----------------------------------------------
# used during setup of pipeline
req_library <- c(
"targets",
"tarchetypes",
"future",
"future.callr",
"fst",
"renv",
"rlang",
"styler",
"docstring"
)
# used during execution of pipeline
pipeline_library <- c(
"here",
"stringr",
"dplyr",
"tidyr",
"data.table",
"cli",
"glue",
"ggplot2",
"haven", # reading/writing of dta files
"tidyverse", # data manipulation/wrangeling
"magrittr", # two sided pipe
"fst", #
"modelsummary"
)
suppressPackageStartupMessages({
# used during setup of pipeline
library(targets)
library(tarchetypes)
library(future)
library(future.callr)
library(fst)
library(renv)
library(rlang)
library(styler)
library(docstring)
# used during execution of pipeline
library(here)
library(stringr)
library(dplyr)
library(tidyr)
library(data.table)
library(cli)
library(glue)
library(ggplot2)
library(haven)
library(modelsummary)
})
tar_load(combined_federal_states)
datasummary_skim(combined_federal_states)
?datasummary_skim
table(combined_federal_states$blid, combined_federal_states$non_list_reason)
install.packaes("janitor")
install.packages("janitor")
library(janitor)
tabyl(combined_federal_states$blid, combined_federal_states$non_list_reason)
combined_federal_states |>  tabyl(blid, non_list_reason)
tst = combined_federal_states |>  head( n = 1000)
View(tst)
tst_filter = filter(combined_federal_states, latlon_utm == "5984007.07143626620541.046182232")
View(tst_filter)
geo_grouped_data = tst_filter
# arrange data and prep for grouping
# use index instead ?
setDT(geo_grouped_data, key = c("latlon_utm", "balkon", "amonths"))
# extract end_date of data
# there should be a better place for this?
data_end_date <- geo_grouped_data[, max(emonths)]
tar_load_globals()
tar_load_globals()
pre_removal_obs <- geo_grouped_data[, .N]
logger::log_info("Pre Removal Observations: ", pre_removal_obs)
geo_grouped_data_similarity <- geo_grouped_data[, similarity_classification(.SD), by = c("latlon_utm", "balkon")]
View(geo_grouped_data_similarity)
geo_grouped_data = tst_filter
View(tst_filter)
geo_grouped_data[, c("non_list_reason","non_list_duration","sim_index","sim_dist","first_occurence_ids","parent"):=NULL]
geo_grouped_data_similarity <- geo_grouped_data[, similarity_classification(.SD), by = c("latlon_utm", "balkon")]
View(geo_grouped_data_similarity)
tar_load(federal_states)
geo_grouped_data = federal_states |> filter(latlon_utm == "5984007.07143626620541.046182232")
# arrange data and prep for grouping
# use index instead ?
setDT(geo_grouped_data, key = c("latlon_utm", "balkon", "amonths"))
# extract end_date of data
# there should be a better place for this?
data_end_date <- geo_grouped_data[, max(emonths)]
pre_removal_obs <- geo_grouped_data[, .N]
View(geo_grouped_data)
logger::log_info("Pre Removal Observations: ", pre_removal_obs)
geo_grouped_data_similarity <- geo_grouped_data[, similarity_classification(.SD), by = c("latlon_utm", "balkon")]
# everything was classified
tar_assert_true(pre_removal_obs == geo_grouped_data_similarity[, .N])
# .SDcols = c("amonths","emonths")
geo_grouped_data_connected <- geo_grouped_data_similarity[, non_list_classification(.SD, data_end_date), by = parent][!"Update", on = "non_list_reason"]
View(geo_grouped_data)
View(geo_grouped_data_similarity)
# find duplicates
duplicates <- duplicated(geo_grouped_data[, ..categories])
# extract ids and combinations of non-duplicates
first_occurence_ids <- geo_grouped_data[!duplicates, counting_id]
# does order matter here? key = amonths?
unique_combinations <- geo_grouped_data[!duplicates, ..categories]
View(unique_combinations)
# assign id to exact duplicate combination based on first occurrence
geo_grouped_data = copy(geo_grouped_data)[
cbind(first_occurence_ids,unique_combinations),
on = categories
]
View(geo_grouped_data)
geo_grouped_data = federal_states |> filter(latlon_utm == "5984007.07143626620541.046182232")
# find duplicates
duplicates <- duplicated(geo_grouped_data[, ..categories])
# extract ids and combinations of non-duplicates
first_occurence_ids <- geo_grouped_data[!duplicates, counting_id]
# does order matter here? key = amonths?
unique_combinations <- geo_grouped_data[!duplicates, ..categories]
# arrange data and prep for grouping
# use index instead ?
setDT(geo_grouped_data, key = c("latlon_utm", "balkon", "amonths"))
# extract end_date of data
# there should be a better place for this?
data_end_date <- geo_grouped_data[, max(emonths)]
pre_removal_obs <- geo_grouped_data[, .N]
# find duplicates
duplicates <- duplicated(geo_grouped_data[, ..categories])
# extract ids and combinations of non-duplicates
first_occurence_ids <- geo_grouped_data[!duplicates, counting_id]
# does order matter here? key = amonths?
unique_combinations <- geo_grouped_data[!duplicates, ..categories]
# assign id to exact duplicate combination based on first occurrence
geo_grouped_data = copy(geo_grouped_data)[
cbind(first_occurence_ids,unique_combinations),
on = categories
]
View(geo_grouped_data)
setkey(geo_grouped_data, wohnflaeche, etage, zimmeranzahl)
similarity_index_list <- similarity_dist_list <- list()
if(!nrow(unique_combinations) == 1){
# increase etage by one to avoid scaling around 0
unique_combinations[, etage := etage + 1]
## similiarity calculations
for (i in 1:nrow(unique_combinations)) {
# percentage of rooms scaled values are allowed to be off
# e.g. what percentage of 8 rooms is 0.5 rooms
# this feels way to complicated
scaled_zimmeranzahl_r_o <- unique_combinations[
,
(as.numeric(unique_combinations[i, "zimmeranzahl"]) + zimmeranzahl_r_o) / as.numeric(unique_combinations[i, "zimmeranzahl"]) - 1
]
# scaling around 0 causes div by 0 and inf for everything else
# just increase etage by one? other solution?
data_to_similarity <- scale(unique_combinations, center = F, scale = unique_combinations[i]) |> as.data.table()
similarity_index_list[[i]] <- data_to_similarity[
,
.(fcase(
## exact repeat [small percentage deviation acceptable]
abs(1 - wohnflaeche) <= wohnflaeche_e_o &
# zimmeranzahl and etage are exact matches
zimmeranzahl == 1 &
etage == 1,
0,
## similar repeat [larger percentage deviation acceptable]
abs(1 - wohnflaeche) <= wohnflaeche_r_o &
# zimmeranzahl deviation acceptable / etage arbitrary
abs(1 - zimmeranzahl) <= scaled_zimmeranzahl_r_o &
# etage exact match
etage == 1,
1,
# no matches
default = NA
))
] |> as.matrix()
similarity_dist_list[[i]] <- as.matrix(dist(data_to_similarity, method = "euclidean"))[i, ]
}
## clustering
# transform to data.tables and set counting ids as column names
similarity_dist_list <- as.data.table(similarity_dist_list)
similarity_index_list <- as.data.table(similarity_index_list)
# enforce zero diagonal(allows classification of zero observations which otherwise are fully NA)
diag(similarity_index_list) = 0
setnames(similarity_index_list, as.character(first_occurence_ids))
setnames(similarity_dist_list, as.character(first_occurence_ids))
# setup and run the actual clustering
clustering <- cluster$new(
# since apply performs action on rows while all other functions use columns
cluster_options = similarity_index_list,
distance = similarity_dist_list*similarity_index_list,
means = rowMeans(similarity_index_list * similarity_dist_list, na.rm = T)
)
clustering$determine_cluster_centers()
if (anyDuplicated(clustering$centers$counting_id)) {
# filter/fix duplicates within $centers here if they exist
# currently its always being parents > being a child to ease calc
# otherwise there has to be a cost assigned for non-parenthood
clustering$centers <- clustering$centers[
,
.SD[which.min(sim_dist)],
by = "counting_id"
]
}
} else {
# this could be function since im doing it more than once
# does this make the if within cluster-class irrelevant?
clustering <- cluster$new(
cluster_options = NULL
)
clustering$centers = data.table(
"counting_id" = as.numeric(first_occurence_ids),
"parent" = as.numeric(first_occurence_ids),
"sim_dist" = 0,
"sim_index" = 0
)
}
View(similarity_dist_list)
View(similarity_index_list)
clustering$centers
# setup and run the actual clustering
clustering <- cluster$new(
# since apply performs action on rows while all other functions use columns
cluster_options = similarity_index_list,
#*similarity_index_list
distance = similarity_dist_list,
means = rowMeans(similarity_index_list * similarity_dist_list, na.rm = T)
)
clustering$determine_cluster_centers()
clustering$centers
if (anyDuplicated(clustering$centers$counting_id)) {
# filter/fix duplicates within $centers here if they exist
# currently its always being parents > being a child to ease calc
# otherwise there has to be a cost assigned for non-parenthood
clustering$centers <- clustering$centers[
,
.SD[which.min(sim_dist)],
by = "counting_id"
]
}
# make sure every combination got clustered
tar_assert_true(nrow(clustering$centers) == nrow(unique_combinations))
# merge cluster results to inital data and return
out <- geo_grouped_data[
clustering$centers,
on = .(first_occurence_ids = counting_id),
allow.cartesian = T
]
similarity_classification <- function(geo_grouped_data = NA) {
#' @title WIP
#'
#' @description WIP
#' @param WIP
#' @param WIP
#' @note
#'
#' @return WIP
#' @author Thorben Wiebe
#----------------------------------------------
# backup = geo_grouped_data
# not working
# geo_grouped_data = classification_77a61db3[latlon_utm == "5875229.20871175489859.109419092"]
# example of overlapping parents/children
#geo_grouped_data = federal_state |>  filter(latlon_utm == "5914872.28545209603584.936611244") |> setDT()
## Preperation
# find duplicates
duplicates <- duplicated(geo_grouped_data[, ..categories])
# extract ids and combinations of non-duplicates
first_occurence_ids <- geo_grouped_data[!duplicates, counting_id]
# does order matter here? key = amonths?
unique_combinations <- geo_grouped_data[!duplicates, ..categories]
# assign id to exact duplicate combination based on first occurrence
geo_grouped_data = copy(geo_grouped_data)[
cbind(first_occurence_ids,unique_combinations),
on = categories
]
setkey(geo_grouped_data, wohnflaeche, etage, zimmeranzahl)
similarity_index_list <- similarity_dist_list <- list()
if(!nrow(unique_combinations) == 1){
# increase etage by one to avoid scaling around 0
unique_combinations[, etage := etage + 1]
## similiarity calculations
for (i in 1:nrow(unique_combinations)) {
# percentage of rooms scaled values are allowed to be off
# e.g. what percentage of 8 rooms is 0.5 rooms
# this feels way to complicated
scaled_zimmeranzahl_r_o <- unique_combinations[
,
(as.numeric(unique_combinations[i, "zimmeranzahl"]) + zimmeranzahl_r_o) / as.numeric(unique_combinations[i, "zimmeranzahl"]) - 1
]
# scaling around 0 causes div by 0 and inf for everything else
# just increase etage by one? other solution?
data_to_similarity <- scale(unique_combinations, center = F, scale = unique_combinations[i]) |> as.data.table()
similarity_index_list[[i]] <- data_to_similarity[
,
.(fcase(
## exact repeat [small percentage deviation acceptable]
abs(1 - wohnflaeche) <= wohnflaeche_e_o &
# zimmeranzahl and etage are exact matches
zimmeranzahl == 1 &
etage == 1,
0,
## similar repeat [larger percentage deviation acceptable]
abs(1 - wohnflaeche) <= wohnflaeche_r_o &
# zimmeranzahl deviation acceptable / etage arbitrary
abs(1 - zimmeranzahl) <= scaled_zimmeranzahl_r_o &
# etage exact match
etage == 1,
1,
# no matches
default = NA
))
] |> as.matrix()
similarity_dist_list[[i]] <- as.matrix(dist(data_to_similarity, method = "euclidean"))[i, ]
}
## clustering
# transform to data.tables and set counting ids as column names
similarity_dist_list <- as.data.table(similarity_dist_list)
similarity_index_list <- as.data.table(similarity_index_list)
# enforce zero diagonal(allows classification of zero observations which otherwise are fully NA)
diag(similarity_index_list) = 0
setnames(similarity_index_list, as.character(first_occurence_ids))
setnames(similarity_dist_list, as.character(first_occurence_ids))
# setup and run the actual clustering
clustering <- cluster$new(
# since apply performs action on rows while all other functions use columns
cluster_options = similarity_index_list,
#*similarity_index_list
distance = similarity_dist_list,
means = rowMeans(similarity_index_list * similarity_dist_list, na.rm = T)
)
clustering$determine_cluster_centers()
if (anyDuplicated(clustering$centers$counting_id)) {
# filter/fix duplicates within $centers here if they exist
# currently its always being parents > being a child to ease calc
# otherwise there has to be a cost assigned for non-parenthood
clustering$centers <- clustering$centers[
,
.SD[which.min(sim_dist)],
by = "counting_id"
]
}
} else {
# this could be function since im doing it more than once
# does this make the if within cluster-class irrelevant?
clustering <- cluster$new(
cluster_options = NULL
)
clustering$centers = data.table(
"counting_id" = as.numeric(first_occurence_ids),
"parent" = as.numeric(first_occurence_ids),
"sim_dist" = 0,
"sim_index" = 0
)
}
# Unit-Test ---------------------------------------------------------------
# make sure every combination got clustered
tar_assert_true(nrow(clustering$centers) == nrow(unique_combinations))
# merge cluster results to inital data and return
out <- geo_grouped_data[
clustering$centers,
on = .(first_occurence_ids = counting_id),
allow.cartesian = T
]
# make sure we dont output more obs than we input
tar_assert_true(nrow(geo_grouped_data) == nrow(out))
return(out)
}
geo_grouped_data_similarity <- geo_grouped_data[, similarity_classification(.SD), by = c("latlon_utm", "balkon")]
geo_grouped_data = federal_states |> filter(latlon_utm == "5984007.07143626620541.046182232")
# arrange data and prep for grouping
# use index instead ?
setDT(geo_grouped_data, key = c("latlon_utm", "balkon", "amonths"))
# extract end_date of data
# there should be a better place for this?
data_end_date <- geo_grouped_data[, max(emonths)]
pre_removal_obs <- geo_grouped_data[, .N]
logger::log_info("Pre Removal Observations: ", pre_removal_obs)
geo_grouped_data_similarity <- geo_grouped_data[, similarity_classification(.SD), by = c("latlon_utm", "balkon")]
# everything was classified
tar_assert_true(pre_removal_obs == geo_grouped_data_similarity[, .N])
View(geo_grouped_data_similarity)
# .SDcols = c("amonths","emonths")
geo_grouped_data_connected <- geo_grouped_data_similarity[, non_list_classification(.SD, data_end_date), by = parent][!"Update", on = "non_list_reason"]
parent_grouped_data = geo_grouped_data_similarity
# this is fairly ugly, but cant reference non_list_duration in second mutation due to data.table not finding it
# throws error without copy due to using := within .SD
parent_grouped_data_non_list <- copy(parent_grouped_data)[
,
non_list_duration := fifelse(
is.na(shift(amonths, 1, type = "lead") - emonths),
data_end_date - emonths,
shift(amonths, 1, type = "lead") - emonths
)
][
,
# update wording might be missleading since the actual update is the subsequent one which actually sold
# updated/overwrite/super-ceded?
non_list_reason := .(fcase(
non_list_duration < 0, "Miss",
non_list_duration >= 0 & non_list_duration <= 6, "Update",
non_list_duration > 6, "Sold"
))
]
View(parent_grouped_data)
View(parent_grouped_data_non_list)
parent_grouped_data = parent_grouped_data |>  filter(parent == "1")
# this is fairly ugly, but cant reference non_list_duration in second mutation due to data.table not finding it
# throws error without copy due to using := within .SD
parent_grouped_data_non_list <- copy(parent_grouped_data)[
,
non_list_duration := fifelse(
is.na(shift(amonths, 1, type = "lead") - emonths),
data_end_date - emonths,
shift(amonths, 1, type = "lead") - emonths
)
][
,
# update wording might be missleading since the actual update is the subsequent one which actually sold
# updated/overwrite/super-ceded?
non_list_reason := .(fcase(
non_list_duration < 0, "Miss",
non_list_duration >= 0 & non_list_duration <= 6, "Update",
non_list_duration > 6, "Sold"
))
]
View(parent_grouped_data_non_list)
data_end_date = 25000
# this is fairly ugly, but cant reference non_list_duration in second mutation due to data.table not finding it
# throws error without copy due to using := within .SD
parent_grouped_data_non_list <- copy(parent_grouped_data)[
,
non_list_duration := fifelse(
is.na(shift(amonths, 1, type = "lead") - emonths),
data_end_date - emonths,
shift(amonths, 1, type = "lead") - emonths
)
][
,
# update wording might be missleading since the actual update is the subsequent one which actually sold
# updated/overwrite/super-ceded?
non_list_reason := .(fcase(
non_list_duration < 0, "Miss",
non_list_duration >= 0 & non_list_duration <= 6, "Update",
non_list_duration > 6, "Sold"
))
]
# set index to allow for binary operations here and later
setindex(parent_grouped_data_non_list, non_list_reason)
parent_grouped_data_connected <- parent_grouped_data_non_list[
# non_list_reason ==
!"Miss",
c("start_position", "end_position") := which_range(non_list_reason),
on = "non_list_reason"
][
,
# can calculate something like price difference from initial offering to actual sale price
amonths := fifelse(
!start_position == end_position & !is.na(start_position),
amonths[start_position],
amonths
)
]
View(parent_grouped_data_connected)
parent_grouped_data_connected[, c("start_position","end_position") := NULL]
# .SDcols = c("amonths","emonths")
geo_grouped_data_connected <- geo_grouped_data_similarity[, non_list_classification(.SD, data_end_date), by = parent]
View(geo_grouped_data_connected)
View(geo_grouped_data_connected)
geo_grouped_data_connected[!"Update", on = "non_list_reason"]
tar_make()
tar_make()
renv::install("kableExtra")
tar_make()
tar_make()
