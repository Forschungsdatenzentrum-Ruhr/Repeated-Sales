---
title: "Repeated sales/rent walkthrough"
output:
  pdf_document: default
  html_document:
    code_folding: show
    theme:
      bg: '#202123'
      fg: '#B8BCC2'
      primary: '#EA80FC'
      secondary: '#00DAC6'
      base_font:
        google: Prompt
      heading_font:
        google: Proza Libre
---
THIS FILE ONLY COMPILES DURING THE PIPELINE; ERRORS IF DONE MANUALLY!

# Introduction

This markdown is a more lengthy explanation of the process behind the clustering of repeated sales. It explains the algorithm more generally and outlines the motivation behind some choices. The individual steps are taken directly from the code base, so they copy what the pipeline does.

# Data And Running

Details of reading/reprocessing the RED data can be found in R/read_/, specifically read_RED.R and prepare_RED.R. These should be self explanatory. Note that the classification algorithm is designed to be run in parallel.^[I recommend using 'tar_make_future(workers = n)', where $n$ is the number of cores. $n = 4$ is a decent value, when no one else is using the server the number can be set higher. More than 8 is overkill, since Berlin alone typically bottlenecks. If speed becomes a big issue with growing data consider using "foreach()" on the coordinate level.] This is achieved by grouping the RED data on "blid", i.e. one group for each federal state. These groups are than classified in parallel for significant speedups. Note that this by default is possible for multiple object types (WK,HK,WM) and can be set via the "static_RED_types" variable. The branching over this argument is dynamic, so only the variable has to be set, the rest of the pipeline adjusts itself. 

The classification digs down to coordinate-level, which is where I will start explaining the actual procedure. 

## Example Data

The example data is taken from a coordinate with $25$ observations of federal state Bremen. Since the data is already subset to the required dissolution for the next steps, the variables "blid" and "latlon_utm" were dropped beforehand (See R/misc/make_example_markdown_data.R for details).

```{r}
## load example data
tar_load(example_markdown_data)

# rename to match code convention
geo_grouped_data = example_markdown_data

#make this dynamic
#tar_load(example_markdown_data, store ="N:/FDZ/Intern/HiWi-Praktikanten/Mitarbeiter/Thorben/repeated offerings/_targets")
```

### General makeup of the data:
```{r}
# show head
head(example_markdown_data)
# show summary
summary(example_markdown_data)
```

# Legacy notes

The initial design of the algorithm accommodated two types of similarity: resembling and exact. The former allows for slightly larger deviations, the latter is quite restrictive. "r_o" refers to resembling offset and "e_o" to exact offset. This has been deprecated for the time being as resembling offsets ended up being less than $1\%$ of the data, making the added complexity not worth it.

## parameter used for classification:
```{r parameters}
# these are globally defined in _targets.R
print(exportJSON)
```

# The problem

The main question this entire project seeks to answer is: which listings can be grouped together, since they likely to refer to the same object (as in apartment or house)? The idea is basically to match the most similar listings found on the coordinates based on their characteristics. To make this decision for any number of characteristic combinations, I use a modified version of k-nearest centroids clustering. \@ref(fig:problem_plot) shows a scatter plot the example data in 3d, illustrating the three main dimensions: etage, wohnflaeche and zimmeranzahl. The plot summarizes the issue in answering the question quite well, as it can be quite difficult to visually identify which points might stem from the same object. Take for example the pair of points located around (2,100,4). It is quite challenging to make a decision whether or not these listings actually refer to the same object (with some measurement deviation in wohnflaeche). This is doubly so since the scatterplot does not show the presence of direct overlaps, which would be a helpful indication of a cluster. So lets move into a more formal level using the actual listings. We will proceed in two overarching dimensions: the characteristics dimension (classifying similarity) and the subsequent time dimension ('classifying' non list reason).  

```{r problem_plot, fig.cap= "Illustration of the Problem"}
# make color blind friendly palette
pal = MetBrewer::met.brewer("Egypt", n = length(example_markdown_data$etage))
with(
  example_markdown_data,
  scatterplot3d(x = wohnflaeche , y = zimmeranzahl, z =  etage , color = pal, pch = 16)
)

```

# characteristics dimension
The goal of this dimension is to find out which listings are similar to one another in terms of how close their characteristics are to one another. As mentioned above, I use three main dimensions: etage, wohnflaeche and zimmeranzahl. These are all categorized by immoscout to be mandatory information and typically do not change much over time^[Other than measurement errors and renovations, which is addressed by allowing small deviations. For the current deviations see \@ref(sec:parameters)], which makes them ideal variables to base the classification off. 

## preperation and unique
To achieve this, the euclidean distance between the scaled values is used as mediator. But as a first step we can save some computations by reducing the data to only the unique combinations of the dimensions, as the distances is constant in those parameters. Another step done for the same reason is subsetting of columns and indexing. The latter allows data.table to perform merges using binary search, which is considerably faster than normal. I use a lot more merges than normal due to this, since its the most efficient way to perform the operations.
```{r}
    # make copy to modifiy keys
    geo_grouped_data <- copy(geo_grouped_data)
    setkey(geo_grouped_data, counting_id)
    setindex(geo_grouped_data, wohnflaeche, etage, zimmeranzahl)
    
    # extract ids and combinations of non-duplicates
    occurence_ids <- geo_grouped_data[, counting_id]
    
    # extract all combinations of categories
    var_to_keep = c(categories, "counting_id")
    combinations <- geo_grouped_data[, ..var_to_keep]

    # filter out duplicates
    # this has to be done ignoring the counting_id, as it is unique
    dup_combinations = duplicated(combinations[, ..categories])
    unique_combinations = combinations[!dup_combinations, ..categories]
    unique_occurence_ids = combinations[!dup_combinations, counting_id]


```
## similarity lists 
The function "make_similarity_lists" returns to lists, the first containing the similarity indices and the second the corresponding scaled euclidean distances (henceforth just similarity distance) between each of the unique combinations.
```{r}
      similarity_lists = make_similarity_lists(unique_combinations,unique_occurence_ids)
      
      similarity_index_list = similarity_lists[[1]]
      similarity_dist_list = similarity_lists[[2]]
      
      similarity_dist_list[is.na(similarity_index_list)] = NA
      
```
### similarity index
The former indicates whether or not a listing is similar (according to the definitions set in \@ref(sec:parameters))
to any of the other listings. NA's indicate no similarity, 0 indicate similarity. Be default, all listings are similar to themselves, making the matrix diagonal zero.

### similarity distances
For convenience, all distances are set to NA where the index is also NA.This just makes the matrix easier to look at.
```{r}
similarity_dist_list = similarity_lists[[2]]
similarity_dist_list[1:5,1:5]
```
### clustering
```{r}
    similarity_dist_list[is.na(similarity_index_list)] = NA
      
    # setup and run the actual clustering
    clustering <- cluster$new(
      cluster_options = similarity_index_list,
      distance = similarity_dist_list
    )
    clustering$determine_cluster_centers()
    clustering$centers |> head(n = 10)
```
```{r}
clustering$centers <- clustering$centers[
        ,
        similarity_cost_function(.SD)
      ]
clustering$centers |>  head(n = 10)
```

```{r}
# example_markdown_data is equvialent to geo_grouped_data in the code
out <- example_markdown_data[
    clustering$centers,
    on = .(counting_id)
  ]
out |>  head(n = 10)
```

