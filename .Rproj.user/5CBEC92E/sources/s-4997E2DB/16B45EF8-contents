removal <- function(geo_grouped_data = NA) {
  #' @title WIP
  #'
  #' @description WIP
  #' @param WIP
  #' @param WIP
  #' @note
  #'
  #' @return WIP
  #' @author Thorben Wiebe
  #----------------------------------------------

  # backup = geo_grouped_data

  # not working
  # geo_grouped_data = classification_77a61db3[latlon_utm == "5875229.20871175489859.109419092"]

  # example of overlapping parents/children
  #geo_grouped_data = federal_state |>  filter(latlon_utm == "5914872.28545209603584.936611244") |> setDT()


  ## Preperation
  # find duplicates
  duplicates <- duplicated(geo_grouped_data[, ..categories])

  # extract ids and combinations of non-duplicates
  first_occurence_ids <- geo_grouped_data[!duplicates, counting_id]
  unique_combinations <- geo_grouped_data[!duplicates, ..categories]

  setkey(unique_combinations, wohnflaeche, etage, zimmeranzahl)

  similarity_index_list <- similarity_dist_list <- list()


  ## similiarity calculations
  for (i in 1:nrow(unique_combinations)) {
    
    # percentage of rooms scaled values are allowed to be off
    # e.g. what percentage of 8 rooms is 0.5 rooms
    # this feels way to complicated
    scaled_zimmeranzahl_r_o <- unique_combinations[
      ,
      (as.numeric(unique_combinations[i, "zimmeranzahl"]) + zimmeranzahl_r_o) / as.numeric(unique_combinations[i, "zimmeranzahl"]) - 1
    ]

    data_to_similarity <- scale(unique_combinations, center = F, scale = unique_combinations[i]) |> as.data.table()

    similarity_index_list[[i]] <- data_to_similarity[
      ,
      .(fcase(

        ## exact repeat [small percentage deviation acceptable]
        abs(1 - wohnflaeche) <= wohnflaeche_e_o &
          # zimmeranzahl and etage are exact matches
          zimmeranzahl == 1 &
          etage == 1,
        0,

        ## similar repeat [larger percentage deviation acceptable]
        abs(1 - wohnflaeche) <= wohnflaeche_r_o &
          # zimmeranzahl deviation acceptable / etage arbitrary
          abs(1 - zimmeranzahl) <= scaled_zimmeranzahl_r_o,
        1,

        # no matches
        default = NA
      ))
    ] |> as.matrix()

    similarity_dist_list[[i]] <- as.matrix(dist(data_to_similarity, method = "euclidean"))[i, ]
  }

  ## clustering
  # transform to data.tables and set counting ids as column names
  similarity_dist_list <- as.data.table(similarity_dist_list)
  similarity_index_list <- as.data.table(similarity_index_list)
  
  # enforce zero diagonal( allows classification of zero wohnraum observations which otherwise are fully NA)
  diag(similarity_index_list) = 0
  
  setnames(similarity_index_list, as.character(first_occurence_ids))
  setnames(similarity_dist_list, as.character(first_occurence_ids))

  # setup and run the actual clustering
  clustering <- cluster$new(
    cluster_options = similarity_index_list,
    distance = similarity_dist_list,
    means = rowMeans(similarity_index_list * similarity_dist_list, na.rm = T)
  )
  clustering$determine_cluster_centers()


  if (anyDuplicated(clustering$centers$counting_id)) {
    
    # filter/fix duplicates within $centers here if they exist
    # currently its always being parents > being a child to ease calc
    # otherwise there has to be a cost assigned for non-parenthood
    
    clustering$centers <- clustering$centers[
      ,
      .SD[which.min(sim_dist)],
      by = "counting_id"
    ]
  }
  
if(!nrow(clustering$centers) == nrow(unique_combinations)){
  #print(clustering$centers)
  print(similarity_index_list)
}


 
  # make sure every combination got clustered
  tar_assert_true(nrow(clustering$centers) == nrow(unique_combinations))
  
  # merge cluster results to inital data and return
  out <- geo_grouped_data[
    clustering$centers,
    on = "counting_id",
    allow.cartesian = T
  ]


  return(out)
}
